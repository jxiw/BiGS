# BiGS

<img width="537" alt="BiGS" src="https://user-images.githubusercontent.com/16102460/221464744-06b6538a-7e84-4c95-909f-239eab1dba71.png">

This repository contains a library for MultiHead Bidirectional Gated State Space Model (BiGS). BiGS combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. So far,  BiGS is able to match BERT pretraining accuracy on GLUE.

# Citation

```
@article{wang2022pretraining,
  title={Pretraining Without Attention},
  author={Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M},
  journal={arXiv preprint arXiv:2212.10544},
  year={2022}
}
```